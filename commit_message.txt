Ramin Yazdani | ML Model Selection Notebook | main | feat(nonlinear): Implement polynomial regression for non-linear patterns

Move beyond linear models by implementing polynomial regression with various degrees to capture non-linear relationships.

This commit introduces:
- Polynomial feature transformation using sklearn's PolynomialFeatures
- Polynomial regression with degrees 2, 4, 5, and 6
- Visual comparison of different polynomial fits
- Analysis of overfitting vs. underfitting trade-offs

Polynomial regression extends linear regression by:
- Creating polynomial combinations of features
- Allowing non-linear relationships to be modeled
- Maintaining linear regression's computational efficiency
- Demonstrating the bias-variance trade-off

Degrees explored:
- Degree 2: Quadratic terms, moderate flexibility
- Degree 4: Higher-order terms, more complex curves
- Degree 5: Even more flexibility, risk of overfitting
- Degree 6: Very flexible, high overfitting risk on training data

Key demonstrations:
- Underfitting: Lower degrees may miss data patterns
- Good fit: Moderate degrees capture true relationships
- Overfitting: Higher degrees fit noise in training data
- Model complexity trade-offs in practice

The visualizations clearly show:
- How increasing polynomial degree affects fit quality
- When additional complexity helps vs. hurts
- Visual evidence of the bias-variance trade-off
- Why model selection matters for non-linear data

This is the first true "beyond linearity" technique, preparing for more sophisticated non-linear methods like splines.

Files modified:
- ml_model_selection_analysis.ipynb - Extended to cells 0-32

Verification: Polynomial regression successfully implemented for all degrees. Visualizations clearly show progression from underfitting to overfitting. Model fits are reasonable and demonstrate the intended concepts.