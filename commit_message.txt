Ramin Yazdani | ML Model Selection Notebook | main | feat(regression): Add Lasso regression for feature selection

Implement Lasso (L1) regression to complement Ridge and demonstrate feature selection capabilities.

This commit adds:
- Lasso regression implementation
- Alpha parameter tuning across multiple values
- Side-by-side comparison with Ridge regression results
- Optimal model selection based on validation performance
- Test set evaluation for final model assessment

Lasso regression features:
- L1 regularization with coefficient penalty
- Sparsity-inducing property (can set coefficients to exactly zero)
- Automatic feature selection capability
- Different regularization behavior compared to Ridge

Key differences from Ridge demonstrated:
- Ridge: Shrinks all coefficients but keeps them non-zero
- Lasso: Can eliminate features entirely by setting coefficients to zero
- Lasso: Useful when you suspect many features are irrelevant
- Ridge: Better when all features contribute somewhat

The comparison shows:
- How different regularization penalties affect model behavior
- Trade-offs between L1 and L2 regularization
- Practical considerations for choosing regularization method
- Impact on test set performance

Alpha values tested include: 0.001, 0.01, 0.1, 1, 10, 100
Best model selected based on lowest MSE and evaluated on held-out test data.

Files modified:
- ml_model_selection_analysis.ipynb - Extended to cells 0-30

Verification: Lasso regression implemented successfully. Optimal alpha identified through cross-validation. Test set MSE computed correctly. Comparison with Ridge shows expected differences in regularization behavior.