Ramin Yazdani | ML Model Selection Notebook | main | feat(regression): Add Ridge regression with regularization

Extend the project to regression tasks by implementing Ridge (L2) regression with hyperparameter tuning.

This commit introduces:
- Synthetic regression data generation
- Ridge regression implementation
- Lambda (alpha) parameter tuning via cross-validation
- Mean Squared Error evaluation
- Regularization path visualization

Ridge regression demonstrates:
- L2 regularization to prevent overfitting
- How penalty term shrinks coefficients
- Impact of hyperparameter (alpha/lambda) on model performance
- Practical hyperparameter tuning workflow

The implementation includes:
- Multiple lambda values tested (0.001, 0.01, 0.1, 1, 10, 100)
- Cross-validation for model selection
- Training set evaluation
- Optimal model identification based on lowest MSE

L2 regularization (Ridge):
- Adds penalty proportional to square of coefficients
- Shrinks coefficients toward zero but never exactly to zero
- Reduces model complexity and variance
- Particularly useful when features are correlated

This begins the "Beyond Linearity" section by first showing how to control model complexity through regularization before moving to non-linear techniques.

Files modified:
- ml_model_selection_analysis.ipynb - Extended to cells 0-24

Verification: Ridge regression runs successfully across all lambda values. MSE computed correctly. Optimal lambda identified. Regularization demonstrates expected behavior of increasing training error as lambda increases.