Ramin Yazdani | ML Model Selection Notebook | main | feat(eval): Evaluate models across different K-fold values

Systematically evaluate all three classification models using different K values to analyze performance stability.

This commit implements:
- K-values evaluation loop testing K = 5, 10, 15, 20, 25, 30
- Performance metric collection for each model and K combination
- Results storage in structured format
- Visualization comparing model performance across K values
- Analysis of how K affects cross-validation results

Key insights demonstrated:
- Model performance stability across different K values
- Trade-offs between computational cost and estimate reliability
- Identification of optimal K value for this dataset
- Comparison of model robustness

The systematic evaluation shows:
- QDA typically outperforms linear models on circular data
- Performance metrics stabilize as K increases
- Different models show different sensitivity to K value
- Higher K values generally give more reliable estimates

This analysis is critical for understanding not just which model performs best, but also how stable and reliable the performance estimates are.

Files modified:
- ml_model_selection_analysis.ipynb - Extended to cells 0-14

Verification: All three models evaluated successfully across all K values. Performance plots generated correctly. QDA shows superior performance as expected for circular data. Metrics are consistent and reasonable.